{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons\n",
    "\n",
    "In this chapter, we will introduce your first truly *deep* networks. \n",
    "The simplest deep networks are called multilayer perceptrons, \n",
    "and they consist of many layers of neurons \n",
    "each fully connected to those in the layer below \n",
    "(from which they receive input) \n",
    "and those above (which they, in turn, influence).\n",
    "When we train high-capacity models we run the risk of overfitting. \n",
    "Thus, we will need to provide your first rigorous introduction\n",
    "to the notions of overfitting, underfitting, and capacity control. \n",
    "To help you combat these problems, \n",
    "we will introduce regularization techniques such as dropout and weight decay.\n",
    "We will also discuss issues relating to numerical stability and parameter initialization that are key to successfully training deep networks. \n",
    "Throughout, we focus on applying models to real data,\n",
    "aiming to give the reader a firm grasp not just of the concepts \n",
    "but also of the practice of using deep networks. \n",
    "We punt matters relating to the computational performance, \n",
    "scalability and efficiency of our models to subsequent chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```eval_rst\n",
    "\n",
    ".. toctree::\n",
    "   :maxdepth: 2\n",
    "\n",
    "   mlp\n",
    "   mlp-scratch\n",
    "   mlp-gluon\n",
    "   underfit-overfit\n",
    "   weight-decay\n",
    "   dropout\n",
    "   backprop\n",
    "   numerical-stability-and-init\n",
    "   environment\n",
    "   kaggle-house-price\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}