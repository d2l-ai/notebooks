{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons\n",
    "\n",
    "In this chapter we will introduce your first truly *deep* networks. The simplest deep networks are called multilayer perceptrons, and they consist of many layers of neurons each fully connected to those in the layer below (from which they receive input) and those above (which they, in turn, influence).\n",
    "When we train high-capacity models we run the risk of overfitting. Thus, we will need to introduce the notions of overfitting, underfitting, and capacity control. Regularization techniques such as dropout, numerical stability and initialization round out the presentation. Throughout, we focus on applying the models to real data, to give the reader a firm grasp not just of the concepts but also of the practice of using deep networks. Issues of performance, scalability and efficiency are relegated to the next chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```eval_rst\n",
    "\n",
    ".. toctree::\n",
    "   :maxdepth: 2\n",
    "\n",
    "   linear-regression\n",
    "   linear-regression-scratch\n",
    "   linear-regression-gluon\n",
    "   softmax-regression\n",
    "   fashion-mnist\n",
    "   softmax-regression-scratch\n",
    "   softmax-regression-gluon\n",
    "   mlp\n",
    "   mlp-scratch\n",
    "   mlp-gluon\n",
    "   underfit-overfit\n",
    "   weight-decay\n",
    "   dropout\n",
    "   backprop\n",
    "   numerical-stability-and-init\n",
    "   environment\n",
    "   kaggle-house-price\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}