{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting House Prices on Kaggle\n",
    "\n",
    "In the previous sections, we introduced \n",
    "the basic tools for building deep networks \n",
    "and performing capacity control via \n",
    "dimensionality-reduction, weight decay and dropout. \n",
    "You are now ready to put all this knowledge into practice\n",
    "by participating in a Kaggle competition. \n",
    "[Predicting house prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) is a great place to start:\n",
    "the data is reasonably generic and doesn't have \n",
    "the kind of rigid structure that might require specialized models\n",
    "the way images or audio might.  \n",
    "This dataset, collected by [Bart de Cock](http://jse.amstat.org/v19n3/decock.pdf) in 2011, is considerably larger than the famous the [Boston housing dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names) of Harrison and Rubinfeld (1978).\n",
    "It boasts both more examples and more features,\n",
    "covering house prices in Ames, IA from the period of 2006-2010. \n",
    "\n",
    "\n",
    "In this section, we will walk you through details of data preprocessing, \n",
    "model design, hyperparameter selection and tuning. \n",
    "We hope that through a hands-on approach,\n",
    "you will be able to observe the effects of capacity control, \n",
    "feature extraction, etc. in practice. \n",
    "This experience is vital to gaining intuition as a data scientist.\n",
    "\n",
    "## Kaggle\n",
    "\n",
    "[Kaggle](https://www.kaggle.com) is a popular platform \n",
    "for machine learning competitions. \n",
    "It combines data, code and users in a way to allow \n",
    "for both collaboration and competition. \n",
    "While leaderboard chasing can sometimes get out of control,\n",
    "there's also a lot to be said for the objectivity in a platform\n",
    "that provides fair and direct quantitative comparisons\n",
    "between your approaches and those devised by your competitors.\n",
    "Moreover, you can checkout the code \n",
    "from (some) other competitors' submissions \n",
    "and pick apart their methods to learn new techniques.\n",
    "If you want to participate in one of the competitions, \n",
    "you need to register for an account (do this now!).\n",
    "\n",
    "![Kaggle website](https://raw.githubusercontent.com/d2l-ai/notebooks/master/img/kaggle.png)\n",
    "\n",
    "On the House Prices Prediction page,\n",
    "you can find the data set (under the data tab), \n",
    "submit predictions, see your ranking, etc.,\n",
    "The URL is right here:\n",
    "\n",
    "> https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "![House Price Prediction](https://raw.githubusercontent.com/d2l-ai/notebooks/master/img/house_pricing.png)\n",
    "\n",
    "## Accessing and Reading Data Sets\n",
    "\n",
    "Note that the competition data is separated into training and test sets. \n",
    "Each record includes the property value of the house \n",
    "and attributes such as street type, year of construction, \n",
    "roof type, basement condition, etc. \n",
    "The features represent multiple datatypes.\n",
    "Year of construction, for example, is represented with integers\n",
    "roof type is a discrete categorical feature,\n",
    "other features are represented with floating point numbers.\n",
    "And here is where reality comes in:\n",
    "for some exampels, some data is altogether missing \n",
    "with the missing value marked simply as 'na'. \n",
    "The price of each house is included for the training set only\n",
    "(it's a competition after all).\n",
    "You can partition the training set to create a validation set,\n",
    "but you'll only find out how you perform on the official test set \n",
    "when you upload your predictions and receive your score.\n",
    "The 'Data' tab on the competition tab has links to download the data.\n",
    "\n",
    "We will read and process the data using `pandas`, \n",
    "an [efficient data analysis toolkit](http://pandas.pydata.org/pandas-docs/stable/), so you will want to make sure that you have `pandas` installed \n",
    "before proceeding further. Fortunately, if you're reading in Jupyter,\n",
    "we can install pandas without even leaving the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies before importing\n",
    "!pip install mxnet-cu100\n",
    "!pip install d2l\n",
    "\n",
    "# If pandas is not installed, please uncomment the following line:\n",
    "# !pip install pandas\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "%matplotlib inline\n",
    "import d2l\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we already downloaded the data \n",
    "and stored it in the `../data` directory. \n",
    "To load the two CSV (Comma Separated Values) files \n",
    "containing training and test data respectively we use Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/kaggle_house_pred_train.csv')\n",
    "test_data = pd.read_csv('../data/kaggle_house_pred_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data set includes 1,460 examples, 80 features, and 1 label.,\n",
    "the test data contains 1,459 examples and 80 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at the first 4 and last 2 features \n",
    "as well as the label (SalePrice) from the first 4 examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "28"
    }
   },
   "outputs": [],
   "source": [
    "train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in each example, the first feature is the ID. \n",
    "This helps the model identify each training example. \n",
    "While this is convenient, it doesn't carry \n",
    "any information for prediction purposes. \n",
    "Hence we remove it from the dataset before feeding the data into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "30"
    }
   },
   "outputs": [],
   "source": [
    "all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "As stated above, we have a wide variety of datatypes. \n",
    "Before we feed it into a deep network,\n",
    "we need to perform some amount of processing. \n",
    "Let's start with the numerical features. \n",
    "We begin by replacing missing values with the mean. \n",
    "This is a reasonable strategy if features are missing at random. \n",
    "To adjust them to a common scale,\n",
    "we rescale them to zero mean and unit variance. \n",
    "This is accomplished as follows:\n",
    "\n",
    "$$x \\leftarrow \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "To check that this transforms $x$ to data \n",
    "with zero mean and unit variance simply calculate \n",
    "$\\mathbf{E}[(x-\\mu)/\\sigma] = (\\mu - \\mu)/\\sigma = 0$. \n",
    "To check the variance we use $\\mathbf{E}[(x-\\mu)^2] = \\sigma^2$ \n",
    "and thus the transformed variable has unit variance. \n",
    "The reason for 'normalizing' the data is that \n",
    "it brings all features to the same order of magnitude. \n",
    "After all, we do not know *a priori* \n",
    "which features are likely to be relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
    "all_features[numeric_features] = all_features[numeric_features].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std()))\n",
    "# After standardizing the data all means vanish, hence we can set missing\n",
    "# values to 0\n",
    "all_features[numeric_features] = all_features[numeric_features].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we deal with discrete values. \n",
    "This includes variables such as 'MSZoning'.\n",
    "We replace them by a one-hot encoding \n",
    "in the same manner as how we transformed multiclass classification data \n",
    "into a vector of $0$ and $1$. \n",
    "For instance, 'MSZoning' assumes the values 'RL' and 'RM'. \n",
    "They map into vectors $(1,0)$ and $(0,1)$ respectively. \n",
    "Pandas does this automatically for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "# Dummy_na=True refers to a missing value being a legal eigenvalue, and\n",
    "# creates an indicative feature for it\n",
    "all_features = pd.get_dummies(all_features, dummy_na=True)\n",
    "all_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this conversion increases the number of features \n",
    "from 79 to 331. \n",
    "Finally, via the `values` attribute,\n",
    " we can extract the NumPy format from the Pandas dataframe \n",
    " and convert it into MXNet's native NDArray representation for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "n_train = train_data.shape[0]\n",
    "train_features = nd.array(all_features[:n_train].values)\n",
    "test_features = nd.array(all_features[n_train:].values)\n",
    "train_labels = nd.array(train_data.SalePrice.values).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "To get started we train a linear model with squared loss. \n",
    "Not surprisingly, our linear model will not lead \n",
    "to a competition winning submission\n",
    "but it provides a sanity check to see whether \n",
    "there's meaningful information in the data.\n",
    "If we can't do better than random guessing here,\n",
    "then there might be a good chance \n",
    "that we have a data processing bug.\n",
    "And if things work, the linear model will serve as a baseline\n",
    "giving us some intuition about how close the simple model\n",
    "gets to the best reported models, giving us a sense \n",
    "of how much gain we should expect from fanicer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "loss = gloss.L2Loss()\n",
    "\n",
    "def get_net():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(1))\n",
    "    net.initialize()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With house prices, as with stock prices, we care about relative quantities more than absolute quantities. \n",
    "More concretely, we tend to care more \n",
    "about the relative error $\\frac{y - \\hat{y}}{y}$ \n",
    "than about the absolute error $y - \\hat{y}$. \n",
    "For instance, if our prediction is off by USD 100,000\n",
    "when estimating the price of a house in Rural Ohio,\n",
    "where the value of a typical house is 125,000 USD,\n",
    "then we are probably doing a horrible job. \n",
    "On the other hand, if we err by this amount in Los Altos Hills, California, \n",
    "this might represent a stunningly accurate prediction \n",
    "(their, the median house price exceeds 4 million USD).\n",
    "\n",
    "One way to address this problem is to \n",
    "measure the discrepancy in the logarithm of the price estimates. \n",
    "In fact, this is also the official error metric\n",
    "used by the compeitition to measure the quality of submissions. \n",
    "After all, a small value $\\delta$ of $\\log y - \\log \\hat{y}$ \n",
    "translates into $e^{-\\delta} \\leq \\frac{\\hat{y}}{y} \\leq e^\\delta$. \n",
    "This leads to the following loss function:\n",
    "\n",
    "$$L = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(\\log y_i -\\log \\hat{y}_i\\right)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "def log_rmse(net, features, labels):\n",
    "    # To further stabilize the value when the logarithm is taken, set the\n",
    "    # value less than 1 as 1\n",
    "    clipped_preds = nd.clip(net(features), 1, float('inf'))\n",
    "    rmse = nd.sqrt(2 * loss(clipped_preds.log(), labels.log()).mean())\n",
    "    return rmse.asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in previous sections, our training functions here \n",
    "will rely on the Adam optimizer \n",
    "(a slight variant on SGD that we will describe in greater detail later).\n",
    "The main appeal of Adam vs vanilla SGD is that \n",
    "the Adam optimizer, despite doing no better (and sometimes worse)\n",
    "given unlimited resources for hyperparameter optimization,\n",
    "people tend to find that it is significantly less sensitive \n",
    "to the initial learning rate. \n",
    "This will be covered in further detail later on \n",
    "when we discuss the details on [Optimization Algorithms](../chapter_optimization/index.md) in a separate chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, train_features, train_labels, test_features, test_labels,\n",
    "          num_epochs, learning_rate, weight_decay, batch_size):\n",
    "    train_ls, test_ls = [], []\n",
    "    train_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "        train_features, train_labels), batch_size, shuffle=True)\n",
    "    # The Adam optimization algorithm is used here\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {\n",
    "        'learning_rate': learning_rate, 'wd': weight_decay})\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        train_ls.append(log_rmse(net, train_features, train_labels))\n",
    "        if test_labels is not None:\n",
    "            test_ls.append(log_rmse(net, test_features, test_labels))\n",
    "    return train_ls, test_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Fold Cross-Validation\n",
    "\n",
    "If you are reading in a linear fashion, \n",
    "you might recall that we intorduced k-fold cross-validation \n",
    "in the section where we discussed how to deal \n",
    "with [“Model Selection, Underfitting and Overfitting\"](underfit-overfit.md). We will put this to good use to select the model design \n",
    "and to adjust the hyperparameters. \n",
    "We first need a function that returns \n",
    "the i-th fold of the data in a k-fold cros-validation procedure. \n",
    "It proceeds by slicing out the i-th segment as validation data \n",
    "and returning the rest as training data. \n",
    "Note that this is not the most efficient way of handling data \n",
    "and we would definitely do something much smarter\n",
    "if our dataset wasconsiderably larger.\n",
    "But this added complexity might obfuscate our code unnecessarily\n",
    "so we can safely omit here owing to the simplicity of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_fold_data(k, i, X, y):\n",
    "    assert k > 1\n",
    "    fold_size = X.shape[0] // k\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = nd.concat(X_train, X_part, dim=0)\n",
    "            y_train = nd.concat(y_train, y_part, dim=0)\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and verification error averages are returned \n",
    "when we train $k$ times in the k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [],
   "source": [
    "def k_fold(k, X_train, y_train, num_epochs,\n",
    "           learning_rate, weight_decay, batch_size):\n",
    "    train_l_sum, valid_l_sum = 0, 0\n",
    "    for i in range(k):\n",
    "        data = get_k_fold_data(k, i, X_train, y_train)\n",
    "        net = get_net()\n",
    "        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n",
    "                                   weight_decay, batch_size)\n",
    "        train_l_sum += train_ls[-1]\n",
    "        valid_l_sum += valid_ls[-1]\n",
    "        if i == 0:\n",
    "            d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'rmse',\n",
    "                         range(1, num_epochs + 1), valid_ls,\n",
    "                         ['train', 'valid'])\n",
    "        print('fold %d, train rmse: %f, valid rmse: %f' % (\n",
    "            i, train_ls[-1], valid_ls[-1]))\n",
    "    return train_l_sum / k, valid_l_sum / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "In this example, we pick an un-tuned set of hyperparameters \n",
    "and leave it up to the reader to improve the model. \n",
    "Finding a good choice can take quite some time, \n",
    "depending on how many things one wants to optimize over.\n",
    "Within reason, the k-fold cross-validation approach \n",
    "is resilient against multiple testing. \n",
    "However, if we were to try out an unreasonably large number of options \n",
    "it might fail since we might just get lucky \n",
    "on the validation split with a particular set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [],
   "source": [
    "k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64\n",
    "train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,\n",
    "                          weight_decay, batch_size)\n",
    "print('%d-fold validation: avg train rmse: %f, avg valid rmse: %f'\n",
    "      % (k, train_l, valid_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that sometimes the number of training errors \n",
    "for a set of hyper-parameters can be very low, \n",
    "while the number of errors for the $K$-fold cross-validation may be higher. This is an indicator that we are overfitting.\n",
    "Therefore, when we reduce the amount of training errors,\n",
    "we need to check whether the amount of errors \n",
    "in the k-fold cross-validation have also been reduced accordingly.\n",
    "\n",
    "##  Predict and Submit\n",
    "\n",
    "Now that we know what a good choice of hyperparameters should be, \n",
    "we might as well use all the data to train on it \n",
    "(rather than just $1-1/k$ of the data \n",
    "that is used in the crossvalidation slices).\n",
    "The model that we obtain in this way \n",
    "can then be applied to the test set. \n",
    "Saving the estimates in a CSV file \n",
    "will simplify uploading the results to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_pred(train_features, test_feature, train_labels, test_data,\n",
    "                   num_epochs, lr, weight_decay, batch_size):\n",
    "    net = get_net()\n",
    "    train_ls, _ = train(net, train_features, train_labels, None, None,\n",
    "                        num_epochs, lr, weight_decay, batch_size)\n",
    "    d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'rmse')\n",
    "    print('train rmse %f' % train_ls[-1])\n",
    "    # Apply the network to the test set\n",
    "    preds = net(test_features).asnumpy()\n",
    "    # Reformat it for export to Kaggle\n",
    "    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n",
    "    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)\n",
    "    submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke our model. \n",
    "One nice sanity check is to see \n",
    "whether the predictions on the test set \n",
    "resemble those of the k-fold cross-validation process. \n",
    "If they do, it's time to upload them to Kaggle.\n",
    "The following code will generate a file called `submission.csv`\n",
    "(CSV is one of the file formats accepted by Kaggle):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [],
   "source": [
    "train_and_pred(train_features, test_features, train_labels, test_data,\n",
    "               num_epochs, lr, weight_decay, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can submit our predictions on Kaggle \n",
    "and see how they compare to the actual house prices (labels) on the test set.\n",
    "The steps are quite simple:\n",
    "\n",
    "* Log in to the Kaggle website and visit the House Price Prediction Competition page.\n",
    "* Click the “Submit Predictions” or “Late Submission” button (as of this writing, the button is located on the right).\n",
    "* Click the “Upload Submission File” button in the dashed box at the bottom of the page and select the prediction file you wish to upload.\n",
    "* Click the “Make Submission” button at the bottom of the page to view your results.\n",
    "\n",
    "![Submitting data to Kaggle](https://raw.githubusercontent.com/d2l-ai/notebooks/master/img/kaggle_submit2.png)\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* Real data often contains a mix of different datatypes and needs to be preprocessed.\n",
    "* Rescaling real-valued data to zero mean and unit variance is a good default. So is replacing missing values with their mean.\n",
    "* Transforming categorical variables into indicator variables allows us to treat them like vectors.\n",
    "* We can use k-fold cross validation to select the model and adjust the hyper-parameters.\n",
    "* Logarithms are useful for relative loss.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Submit your predictions for this tutorial to Kaggle. How good are your predictions?\n",
    "1. Can you improve your model by minimizing the log-price directly? What happens if you try to predict the log price rather than the price?\n",
    "1. Is it always a good idea to replace missing values by their mean? Hint - can you construct a situation where the values are not missing at random?\n",
    "1. Find a better representation to deal with missing values. Hint - What happens if you add an indicator variable?\n",
    "1. Improve the score on Kaggle by tuning the hyperparameters through k-fold crossvalidation.\n",
    "1. Improve the score by improving the model (layers, regularization, dropout).\n",
    "1. What happens if we do not standardize the continuous numerical features like we have done in this section?\n",
    "\n",
    "## Scan the QR Code to [Discuss](https://discuss.mxnet.io/t/2346)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/d2l-ai/notebooks/master/img/qr_kaggle-house-price.png\" alt=\"\" width=75 height=75/>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}