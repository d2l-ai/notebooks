{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Synonyms and Analogies\n",
    "\n",
    "In the [\"Implementation of Word2vec\"](./word2vec-gluon.md) section, we trained a word2vec word embedding model on a small-scale data set and searched for synonyms using the cosine similarity of word vectors. In practice, word vectors pre-trained on a large-scale corpus can often be applied to downstream natural language processing tasks. This section will demonstrate how to use these pre-trained word vectors to find synonyms and analogies. We will continue to apply pre-trained word vectors in later chapters.\n",
    "\n",
    "## Using Pre-trained Word Vectors\n",
    "\n",
    "MXNet's `contrib.text` package provides functions and classes related to natural language processing (see the GluonNLP tool package[1] for more details). Next, we will look at the name of the pre-trained word embeddings it currently provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies before importing\n",
    "!pip install mxnet-cu92\n",
    "!pip install gluonbook\n",
    "\n",
    "from mxnet import nd\n",
    "from mxnet.contrib import text\n",
    "\n",
    "text.embedding.get_pretrained_file_names().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the name of the word embedding, we can see which pre-trained models are provided by the word embedding. The word vector dimensions of each model may be different or obtained by pre-training on different data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "35"
    }
   },
   "outputs": [],
   "source": [
    "print(text.embedding.get_pretrained_file_names('glove'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general naming conventions for pre-trained GloVe models are \"model.(data set.)number of words in data set.word vector dimension.txt\". For more information, please refer to the GloVe and fastText project sites [2,3]. Below, we use a 50-dimensional GloVe word vector based on Wikipedia subset pre-training. The corresponding word vector is automatically downloaded the first time we create a pre-trained word vector instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "glove_6b50d = text.embedding.create(\n",
    "    'glove', pretrained_file_name='glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the dictionary size. The dictionary contains 400,000 words and a special unknown token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a word to get its index in the dictionary, or we can get the word from its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "glove_6b50d.token_to_idx['beautiful'], glove_6b50d.idx_to_token[3367]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Pre-trained Word Vectors\n",
    "\n",
    "Below, we demonstrate the application of pre-trained word vectors, using GloVe as an example.\n",
    "\n",
    "### Seeking Synonyms\n",
    "\n",
    "Here, we re-implement the algorithm used to search for synonyms by cosine similarity introduced in the [\"Implementation of Word2vec\"](./word2vec-gluon.md) section. In order to reuse the logic for seeking the $k$ nearest neighbors when seeking analogies, we encapsulate this part of the logic separately in the `knn` ($k$-nearest neighbors) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(W, x, k):\n",
    "    cos = nd.dot(W, x.reshape((-1,))) / (\n",
    "        nd.sum(W * W, axis=1).sqrt() * nd.sum(x * x).sqrt())\n",
    "    topk = nd.topk(cos, k=k, ret_typ='indices').asnumpy().astype('int32')\n",
    "    return topk, [cos[i].asscalar() for i in topk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we search for synonyms by pre-training the word vector instance `embed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    topk, cos = knn(embed.idx_to_vec,\n",
    "                    embed.get_vecs_by_tokens([query_token]), k+2)\n",
    "    for i, c in zip(topk[2:], cos[2:]):  # Remove input words and unknown words.\n",
    "        print('cosine sim=%.3f: %s' % (c, (embed.idx_to_token[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary of pre-trained word vector instance `glove_6b50d` already created contains 400,000 words and a special unknown token. Excluding input words and unknown words, we search for the three words that are the most similar in meaning to \"chip\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_tokens('chip', 3, glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we search for the synonyms of \"baby\" and \"beautiful\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_tokens('baby', 3, glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_tokens('beautiful', 3, glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeking Analogies\n",
    "\n",
    "In addition to seeking synonyms, we can also use the pre-trained word vector to seek the analogies between words. For example, “man”:“woman”::“son”:“daughter” is an example of analogy, “man” is to “woman” as “son” is to “daughter”. The problem of seeking analogies can be defined as follows: for four words in the analogical relationship $a : b :: c : d$, given the first three words, $a$, $b$ and $c$, we want to find $d$. Assume the word vector for the word $w$ is $\\text{vec}(w)$. To solve the analogy problem, we need to find the word vector that is most similar to the result vector of $\\text{vec}(c)+\\text{vec}(b)-\\text{vec}(a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(token_a, token_b, token_c, embed):\n",
    "    vecs = embed.get_vecs_by_tokens([token_a, token_b, token_c])\n",
    "    x = vecs[1] - vecs[0] + vecs[2]\n",
    "    topk, cos = knn(embed.idx_to_vec, x, 2)\n",
    "    return embed.idx_to_token[topk[1]]  # Remove unknown words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the \"male-female\" analogy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [],
   "source": [
    "get_analogy('man', 'woman', 'son', glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Capital-country” analogy: \"beijing\" is to \"china\" as \"tokyo\" is to what? The answer should be \"japan\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [],
   "source": [
    "get_analogy('beijing', 'china', 'tokyo', glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Adjective-superlative adjective\" analogy: \"bad\" is to \"worst\" and \"big\" is to what? The answer should be \"biggest\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    }
   },
   "outputs": [],
   "source": [
    "get_analogy('bad', 'worst', 'big', glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Present tense verb-past tense verb\" analogy: \"do\" is to \"did\" as \"go\" is to what? The answer should be \"went\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "21"
    }
   },
   "outputs": [],
   "source": [
    "get_analogy('do', 'did', 'go', glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Word vectors pre-trained on a large-scale corpus can often be applied to downstream natural language processing tasks.\n",
    "* We can use pre-trained word vectors to seek synonyms and analogies.\n",
    "\n",
    "\n",
    "## Problems\n",
    "\n",
    "* Test the fastText results. It is worth mentioning that fastText has a pre-trained Chinese word vector (pretrained_file_name='wiki.zh.vec').\n",
    "* If the dictionary is extremely large, how can we improve the synonym and analogy search speed?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Reference\n",
    "\n",
    "[1] GluonNLP tool package. https://gluon-nlp.mxnet.io/\n",
    "\n",
    "[2] GloVe project website. https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "[3] fastText project website. https://fasttext.cc/\n",
    "\n",
    "## Discuss on our Forum\n",
    "\n",
    "[Link to the discuss thread.](https://discuss.mxnet.io/t/2390)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}